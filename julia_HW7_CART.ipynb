{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project 7: CART</h2>\n",
    "\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"forest.jpg\" width=\"400px\" />\n",
    "    </center>\n",
    "      <p><cite><center>Boosting took a long time to be truly understood.<br>\n",
    "      ... cynics say we didn't see the forest for all the trees ...<br>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "<!--announcements-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Introduction</h3>\n",
    "<p>In this assignment you will implement a decision tree algorithm and then use it for bagging and boosting. We've provided a tree structure for you with distinct leaves and nodes. Leaves have two fields, parent (another node) and prediction (a numerical value). Nodes have six fields: \n",
    "\n",
    "<ol>\n",
    "<li> <b>left</b>: node describing left subtree </li>\n",
    "<li> <b>right</b>: node describing right subtree </li>\n",
    "<li> <b>parent</b>: the parent of the current subtree. The head of the tree always has the null pointer <code><b>nothing</b></code> as its parent. Feel free to initialize nodes with this field set to <code><b>nothing</b></code> so long as you set the correct parent later on. </li>\n",
    "<li> <b>feature</b>: index of feature to cut </li>\n",
    "<li> <b>cutoff</b>: cutoff value c (<=c : left, and >c : right)</li>\n",
    "<li> <b>prediction</b>: prediction at this node </li>\n",
    "</ol>\n",
    "\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "type Leaf\n",
    "    parent     :: Union{Void, Any}      # could be optional first\n",
    "    prediction :: Float64\n",
    "end\n",
    "\n",
    "type TreeNode \n",
    "    left       :: Union{TreeNode, Leaf}\n",
    "    right      :: Union{TreeNode, Leaf}\n",
    "    parent     :: Union{Void, TreeNode} # could be optional first\n",
    "    feature    :: Int\n",
    "    cutoff     :: Float64\n",
    "    prediction :: Float64\n",
    "end\n",
    "\n",
    "Tree = Union{TreeNode, Leaf};\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing CART</h3>\n",
    "Before we get started let us add a few packages that you might need. We will also load a data set <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which we will use as our binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try\n",
    "    using Distributions\n",
    "catch\n",
    "    Pkg.add(\"Distributions\") # this contains the command \"wsample\"\n",
    "end;\n",
    "try\n",
    "    using MAT\n",
    "catch\n",
    "    Pkg.add(\"MAT\")     \n",
    "end;\n",
    "try\n",
    "    using PyPlot\n",
    "catch\n",
    "    Pkg.add(\"PyPlot\")     \n",
    "end;\n",
    "\n",
    "# load in some binary test data (labels are -1, +1)\n",
    "using MAT\n",
    "using PyPlot\n",
    "data=matread(\"ion.mat\");\n",
    "xTr=data[\"xTr\"]';\n",
    "yTr=data[\"yTr\"]';\n",
    "xTe=data[\"xTe\"]';\n",
    "yTe=data[\"yTe\"]';\n",
    "\n",
    "# cut off the first two dimensions for visualization\n",
    "xTr2d=xTr[:,1:2]\n",
    "xTe2d=xTe[:,1:2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First, implement the function <code>sqsplit</code> which takes as input a (weighted) data set with labels and computes the best feature and cut-value of an optimal split based on minimum squared error. The third input is a weight vector which assigns a positive weight to each training sample. The loss you should minimize is\n",
    "$$\n",
    "loss(L,R)=\\sum_{i\\in L}w_i\\left(y_i-\\frac{1}{w_{L}}\\mu_L\\right)^2+\\sum_{j\\in R}w_j\\left(y_j-\\frac{1}{w_{R}}\\mu_R\\right)^2\n",
    "$$\n",
    "where\n",
    "$$w_L=\\sum_{i\\in L}w_i \\textrm{ and } \\mu_L=\\sum_{i\\in L}w_i y_i$$ \n",
    "and respectively for $w_R$ and $\\mu_R$. <br>\n",
    "Use your derivation from Homework 5, Problem 2 to implement this function efficiently.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function sqsplit(xTr,yTr;weights=[])\n",
    "    \"\"\"\n",
    "    function (feat,cut,loss)=sqsplit(xTr,yTr;weights)\n",
    "\n",
    "    Uses minimum squared error to compute the optimal feature \n",
    "    and cutoff point for a binary split. Returns the chosen\n",
    "    feature, the cutoff point, and the optimal calculated loss. \n",
    "\n",
    "    Input:\n",
    "        xTr       | nxd input matrix with n column-vectors of dimensionality d\n",
    "        yTr       | nx1 input matrix\n",
    "        weights   | nx1 vector where weights(i) is the weight of example i\n",
    "\n",
    "    Output:\n",
    "        feature   | the index of the feature the split is performed on\n",
    "        cut       | the cutoff value (left <= cut < right)\n",
    "        bestloss  | the squared loss corresponding to the chosen cut\n",
    "    \"\"\"\n",
    "\n",
    "    N,D=size(xTr);\n",
    "    assert(D>0); # must have at least one dimension\n",
    "    assert(N>1); # must have at least two samples\n",
    "    if isempty(weights) # if no weights are passed on, assign uniform weights\n",
    "        weights=ones(N);\n",
    "    end;\n",
    "    weights=weights./sum(weights); # Weights need to sum to one (we just normalize them)\n",
    "\n",
    "    bestloss=Inf;\n",
    "    feature=Inf;\n",
    "    cut=Inf;\n",
    "        \n",
    "    # TODO 1\n",
    "        \n",
    "    assert(feature!=Inf);\n",
    "    assert(cut!=Inf);\n",
    "    return(feature,cut,bestloss)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your <code>sqsplit</code> function using the code below, which finds the first optimal cut in the ION dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic();fid,cut,loss=sqsplit(xTr,yTr);toc();\n",
    "println(\"Output should split on feature 3 at value 0.29920161791103117\");\n",
    "println(\"Split occurs on feature \",fid,\" at value:\",cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Implement the function <code>cart</code> which returns a regression tree based on the minimum squared loss splitting rule. The function takes training data, test data, a maximum depth, and the weight of each training example. Maximum depth and weight are optional arguments. If they are not provided you should make maximum depth infinity and equally weight each example. You should use the function <code>sqsplit</code>.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function cart(xTr,yTr;depth=Inf,weights=nothing)\n",
    "    \"\"\"\n",
    "    function T=cart(xTr,yTr;depth,weights)\n",
    "\n",
    "    The maximum tree depth is defined by \"maxdepth\" (maxdepth=2 means one split).\n",
    "    Each example can be weighted with \"weights\".\n",
    "\n",
    "    Builds a CART tree\n",
    "\n",
    "    Input:\n",
    "        xTr       | nxd input matrix with n column-vectors of dimensionality d\n",
    "        yTr       | nx1 input matrix\n",
    "        depth     | maximum tree depth\n",
    "        weights   | nx1 vector where weights(i) is the weight of example i\n",
    "\n",
    "    Output:\n",
    "        T = decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    n,d=size(xTr);\n",
    "    @assert size(xTr, 1) == size(yTr,1)\n",
    "    if weights == nothing\n",
    "        w=ones(n,1)./n;\n",
    "    else\n",
    "        w=weights\n",
    "    end;\n",
    "    \n",
    "    # TODO 2\n",
    "    \n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Implement the function <code>evaltree.m</code>, which evaluates a decision tree on a given test data set.</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function evaltree(root,xTe)\n",
    "    \"\"\"\n",
    "    function ypredict=evaltree(root,xTe);\n",
    "\n",
    "    input:\n",
    "    root | tree structure\n",
    "    xTe  | Test data (nxd matrix)\n",
    "\n",
    "    output:\n",
    "\n",
    "    y = predictions of labels for xTe\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO 3\n",
    "    \n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your <code>evaltree</code> function using the code below, which computes the root-mean-squared-error of your CART tree on the ION dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic();T=cart(xTr,yTr);toc()\n",
    "errTr=sqrt(mean((evaltree(T,xTr).-yTr).^2));\n",
    "errTe=sqrt(mean((evaltree(T,xTe).-yTe).^2));\n",
    "println(\"Training RMSE: $(round(errTr,2))\");\n",
    "println(\"Testing RMSE: $(round(errTe,2))\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is experimental to visualize the tree\n",
    "function visclassifier(xTr,yTr,classifier;colors=[(0,0,1),(1,0,0),(0,1,0)],symbols = [\"ko\",\"kx\"]);\n",
    "    # plot data\n",
    "    figure();\n",
    "    hold(true);\n",
    "    classvals=unique(yTr);\n",
    "    # plot data points\n",
    "    for c = 1:length(classvals)\n",
    "        idx = find(yTr.==classvals[c]);\n",
    "        hp=scatter(xTr[idx,1], xTr[idx,2],40,colors[c,:],zorder=3);    \n",
    "     end\n",
    "\n",
    "    # plot surface behind them\n",
    "    axis(\"equal\");\n",
    "    axis(\"tight\");\n",
    "    ax=axis();\n",
    "    res=300;\n",
    "    xrange = linspace(ax[1], ax[2],res);\n",
    "    yrange = linspace(ax[3],ax[4],res);\n",
    "    pixelX=repmat(xrange',length(yrange),1);\n",
    "    pixelY=repmat(yrange, 1,length(xrange));\n",
    "    testPoints = [pixelX[:] pixelY[:] ones(length(pixelX[:]),1)];\n",
    "    testLabels=classifier(testPoints);\n",
    "\n",
    "    Z = zeros(res,res);\n",
    "    Z[:]+=testLabels[:];\n",
    "    contourf(pixelX, pixelY, Z,zorder=-1);\n",
    "end;\n",
    "\n",
    "T=cart(xTr2d,yTr); # compute tree on training data \n",
    "h=X->evaltree(T,X);\n",
    "visclassifier(xTr2d,yTr,h);\n",
    "colorbar()\n",
    "println(\"Training classification error: \",mean(sign(h(xTr2d)).!=yTr))\n",
    "println(\"Testing classification error: \",mean(sign(h(xTe2d)).!=yTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forests</h2>\n",
    "<p>CART trees are known to be high variance classifiers (if trained to full depth). An effective way to prevent overfitting is to use <b>Bagging</b>. Implement the function <code>forest</code>, which builds a forest of regression trees. Each tree should be built using training data drawn by randomly sampling n examples from the training data with replacement. You can use the <code>wsample</code> function to do this. Do not randomly sample features. The function should output a set of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Distributions\n",
    "#<GRADED>\n",
    "function forest(xTr,yTr,m;maxdepth=Inf)\n",
    "    \"\"\"\n",
    "    function trees=forest(xTr,yTr,m;maxdepth)\n",
    "    Uses random sampling with replacement to construct \n",
    "    a random forest of size m. \n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        m        | number of trees\n",
    "        maxdepth | maximum depth for all trees\n",
    "    \n",
    "    Output:\n",
    "        trees    | any indexable and iterable collection of CART trees\n",
    "    \"\"\"\n",
    "    n,d=size(xTr);\n",
    "    trees = [];\n",
    "    \n",
    "    # TODO 4\n",
    "    \n",
    "    return(trees);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now implement the function <code>evalforest</code>, which should take as input a set of $m$ trees, a set of $n$ test inputs, and an $m$ dimensional weight vector. Each tree should be weighted by the corresponding weight. (For random forests you can define the weights to be $\\frac{1}{m}$ for all trees).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function evalforest(trees,xTe;αs=[])\n",
    "    \"\"\"\n",
    "    function trees=forest(xTr,yTr,m;maxdepth)\n",
    "    Uses random sampling with replacement to construct \n",
    "    a random forest of size m. \n",
    "    \n",
    "    Input:\n",
    "        trees  | set of CART trees \n",
    "        xTe    | testing data (nxd)\n",
    "        αs     | optional weights corresponding to each tree in trees\n",
    "    \n",
    "    Output:\n",
    "        y      | set of test predictions (nx1)\n",
    "    \"\"\"\n",
    "    n,d=size(xTe);     \n",
    "    if isempty(αs) # if weights are not passed on, set them to uniform\n",
    "        αs=ones(length(trees),1)./length(trees);\n",
    "    end;\n",
    "    output=zeros(n);\n",
    "    \n",
    "    # TODO 5\n",
    "    \n",
    "    return(output)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following script visualizeds the decision boundary of a random forest ensemble.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MAT\n",
    "using PyPlot\n",
    "# load iris data\n",
    "F=forest(xTr2d,yTr,100); # compute tree on training data \n",
    "h=X->evalforest(F,X);\n",
    "visclassifier(xTr2d,yTr,h);\n",
    "colorbar()\n",
    "println(\"Training error:\",mean(sign(h(xTr2d)).!=yTr))\n",
    "println(\"Testing error:\",mean(sign(h(xTe2d)).!=yTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following script evaluates the test and training error of a random forest ensemble as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=matread(\"ion.mat\");\n",
    "xTr=data[\"xTr\"]';\n",
    "yTr=data[\"yTr\"]';\n",
    "xTe=data[\"xTe\"]';\n",
    "yTe=data[\"yTe\"]';\n",
    "M=20; # max number of trees\n",
    "ϵTeF=zeros(M);\n",
    "ϵTrF=zeros(M);\n",
    "for i=1:M\n",
    "    Forest=forest(xTr,yTr,i);\n",
    "    ϵTrF[i]=mean(sign(evalforest(Forest,xTr)).!=yTr[:]);\n",
    "    ϵTeF[i]=mean(sign(evalforest(Forest,xTe)).!=yTe[:]);\n",
    "end;\n",
    "plot(1:M,ϵTrF);\n",
    "hold(true);\n",
    "plot(1:M,ϵTeF);\n",
    "legend([\"Training Error\",\"Testing error\"])\n",
    "xlabel(\"# of trees\")\n",
    "ylabel(\"error\")\n",
    "title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2>Boosting</h2>\n",
    "\n",
    "<p>Another option to improve your decision trees is to build trees of small depth (e.g. only depth=3 or depth=4). These do not have high variance, but instead suffer from <b>high bias</b>. You can reduce the bias of a classifier with boosting. Implement the function <code>boosttree</code>, which applies Adaboost on your <code>cart</code> functions. You should be able to use the function <code>evalforest</code> to evaluate your boosted ensemble (provdided you pass on the weights correctly.)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function boosttree(x,y;maxiter=100,maxdepth=2)\n",
    "    \"\"\"\n",
    "    function trees=boosttree(x,y,nt,maxdepth)\n",
    "\n",
    "    Learns a boosted decision tree on data x with labels y.\n",
    "    It performs at most nt boosting iterations. Each decision tree has maximum depth \"maxdepth\".\n",
    "\n",
    "    INPUT:\n",
    "        x        | input vectors (nxd)\n",
    "        y        | input labels (nx1)\n",
    "        maxiter  | number of trees (default = 100)\n",
    "        maxdepth | depth of each tree (default = 2)\n",
    "\n",
    "    OUTPUT:\n",
    "        trees    | the indexable, iterable set of trees comprising your boosted forest\n",
    "        αs       | the weights corresponding to each tree\n",
    "    \"\"\"\n",
    "\n",
    "    assert(sort(unique(y))==[-1,1]); # the labels must be -1 and 1 \n",
    "    n,d=size(x);\n",
    "    weights=ones(n,1)./n;\n",
    "    preds=zeros(n);\n",
    "    Forest=[];\n",
    "    αs=[];\n",
    "    \n",
    "    # TODO 6\n",
    "    \n",
    "    return(Forest,αs)\n",
    "end;\n",
    "##>>kqw\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following script evaluates the test and training error of a boosted ensemble as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=matread(\"ion.mat\");\n",
    "xTr=data[\"xTr\"]';\n",
    "yTr=data[\"yTr\"]';\n",
    "xTe=data[\"xTe\"]';\n",
    "yTe=data[\"yTe\"]';\n",
    "M=20; # max number of trees\n",
    "ϵTeB=zeros(M);\n",
    "ϵTrB=zeros(M);\n",
    "for i=1:M\n",
    "    (Forest,αs)=boosttree(xTr,yTr,maxdepth=3,maxiter=i);\n",
    "    ϵTrB[i]=mean(sign(evalforest(Forest,xTr;αs=αs)).!=yTr[:]);\n",
    "    ϵTeB[i]=mean(sign(evalforest(Forest,xTe;αs=αs)).!=yTe[:]);\n",
    "end;\n",
    "hold(true);\n",
    "plot(1:M,ϵTrB);\n",
    "plot(1:M,ϵTeB);\n",
    "legend([\"Training Error\",\"Testing error\"])\n",
    "xlabel(\"# of trees\")\n",
    "ylabel(\"error\")\n",
    "title(\"Adaboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(F,αs)=boosttree(xTr2d,yTr,maxdepth=3,maxiter=100); # compute tree on training data \n",
    "h=X->evalforest(F,X;αs=αs);\n",
    "visclassifier(xTr2d,yTr,h);\n",
    "colorbar()\n",
    "println(\"Training error:\",mean(sign(h(xTr2d)).!=yTr))\n",
    "println(\"Testing error:\",mean(sign(h(xTe2d)).!=yTe))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
