{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Project 7: CART</h2>\n",
    "\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"forest.jpg\" width=\"400px\" />\n",
    "    </center>\n",
    "      <p><cite><center>Boosting took a long time to be truly understood.<br>\n",
    "      ... cynics say we didn't see the forest for all the trees ...<br>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "<!--announcements-->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "<p>In this assignment you will implement a decision tree algorithm and then use it for bagging and boosting. We've provided a tree structure for you with distinct leaves and nodes. Leaves have two fields, parent (another node) and prediction (a numerical value). Nodes have six fields: \n",
    "\n",
    "<ol>\n",
    "<li> <b>left</b>: node describing left subtree </li>\n",
    "<li> <b>right</b>: node describing right subtree </li>\n",
    "<li> <b>parent</b>: the parent of the current subtree. The head of the tree always has <code><b>None</b></code> as its parent. Feel free to initialize nodes with this field set to <code><b>None</b></code> so long as you set the correct parent later on. </li>\n",
    "<li> <b>cutoff_id</b>: index of feature to cut </li>\n",
    "<li> <b>cutoff_val</b>: cutoff value c (<=c : left, and >c : right)</li>\n",
    "<li> <b>prediction</b>: prediction at this node </li>\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "class TreeNode(object):\n",
    "    \"\"\"Tree class.\n",
    "    \n",
    "    (You don't need to add any methods or fields here but feel\n",
    "    free to if you like. Our tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, parent, cutoff_id, cutoff_val, prediction):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.parent = parent\n",
    "        self.cutoff_id = cutoff_id\n",
    "        self.cutoff_val = cutoff_val\n",
    "        self.prediction = prediction\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Implementing CART</h3>\n",
    "Before we get started let us add a few packages that you might need. We will also load a data set <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which we will use as our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "#</GRADED>\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "xTr  = data['xTr'].T\n",
    "yTr  = data['yTr'].flatten()\n",
    "xTe  = data['xTe'].T\n",
    "yTe  = data['yTe'].flatten()\n",
    "\n",
    "# cut off the first two dimensions for visualization\n",
    "xTr2d = xTr[:, 0:2]\n",
    "xTe2d = xTe[:, 0:2]\n",
    "\n",
    "xTr.shape, yTr.shape, xTe.shape, yTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>First, implement the function <code>sqsplit</code> which takes as input a (weighted) data set with labels and computes the best feature and cut-value of an optimal split based on minimum squared error. The third input is a weight vector which assigns a positive weight to each training sample. The loss you should minimize is\n",
    "$$\n",
    "loss(L,R)=\\sum_{i\\in L}w_i\\left(y_i-\\frac{1}{w_{L}}\\mu_L\\right)^2+\\sum_{j\\in R}w_j\\left(y_j-\\frac{1}{w_{R}}\\mu_R\\right)^2\n",
    "$$\n",
    "where\n",
    "$$w_L=\\sum_{i\\in L}w_i \\textrm{ and } \\mu_L=\\sum_{i\\in L}w_i y_i$$ \n",
    "and respectively for $w_R$ and $\\mu_R$. <br>\n",
    "Use your derivation from Homework 5, Problem 2 to implement this function efficiently.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def sqsplit(xTr,yTr,weights=[]):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights == []: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    \n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    \n",
    "    \n",
    "    return feature, cut, bestloss\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fid,cut,loss = sqsplit(xTr,yTr)\n",
    "t1 = time.time()\n",
    "print('elapsed time:',t1-t0,'seconds')\n",
    "print(\"It should split on feature 2 on value 0.29920161791103117\")\n",
    "print(\"Split on feature\",fid,\"on value:\",cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Cart tree:</b><p>Implement the function <code>cart</code> which returns a regression tree based on the minimum squared loss splitting rule. The function takes training data, test data, a maximum depth, and the weigh of each training example. Maximum depth and weight are optional arguments. If they are not provided you should make maximum depth infinity and equally weight each example. You should use the function <code>sqsplit</code> to make your splits.</p>\n",
    "\n",
    "<p>Use the provided <code>TreeNode</code> class to represent your tree. Note that the nature of CART trees implies that every node has exactly 0 or 2 children.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def cart(xTr,yTr,depth=np.inf,weights=None):\n",
    "    \"\"\"Builds a CART tree.\n",
    "    \n",
    "    The maximum tree depth is defined by \"maxdepth\" (maxdepth=2 means one split).\n",
    "    Each example can be weighted with \"weights\".\n",
    "\n",
    "    Args:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "        maxdepth: maximum tree depth\n",
    "        weights:  n-dimensional weight vector for data points\n",
    "\n",
    "    Returns:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n,d = xTr.shape\n",
    "    if weights is None:\n",
    "        w = np.ones(n) / float(n)\n",
    "    else:\n",
    "        w = weights\n",
    "        \n",
    "    tree = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Tree\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>Implement the function <code>evaltree</code>, which evaluates a decision tree on a given test data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def evaltree(root,xTe):\n",
    "    \"\"\"Evaluates xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        root: TreeNode decision tree\n",
    "        xTe:  n x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    assert root is not None\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pred\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTr, yTr)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTr) - yTr)**2)\n",
    "te_err   = np.mean((evaltree(root,xTe) - yTe)**2)\n",
    "\n",
    "print(\"elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=[],b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    w = np.array(w).flatten()\n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    testpreds = fun(xTe)\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "\n",
    "    if w != []:\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "    \n",
    "tree=cart(xTr2d,yTr) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X),xTr2d,yTr)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTr2d)) != yTr))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTe2d)) != yTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Random Forests</h2>\n",
    "<p>CART trees are known to be high variance classifiers\n",
    "(if trained to full depth).\n",
    "An effective way to prevent overfitting is to use <b>Bagging</b>.\n",
    "Implement the function <code>forest</code>,\n",
    "which builds a forest of regression trees.\n",
    "Each tree should be built using training data\n",
    "drawn by randomly sampling $n$ examples\n",
    "from the training data with replacement.\n",
    "Do not randomly sample features.\n",
    "The function should output a list of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def forest(xTr, yTr, m, maxdepth=np.inf):\n",
    "    \"\"\"Creates a random forest.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of tree\n",
    "        \n",
    "    Output:\n",
    "        trees: list of TreeNode decision trees of length m\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    \n",
    "\n",
    "    \n",
    "    return trees\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>Now implement the function <code>evalforest</code>, which should take as input a set of $m$ trees, a set of $n$ test inputs, and an $m$ dimensional weight vector. Each tree should be weighted by the corresponding weight. (For random forests you can define the weights to be $\\frac{1}{m}$ for all trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def evalforest(trees, X, alphas=None):\n",
    "    \"\"\"Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of TreeNode decision trees of length m\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / len(trees)\n",
    "            \n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pred\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>The following script visualizes the decision boundary of a random forest ensemble.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trees=forest(xTr2d,yTr,100) # compute tree on training data \n",
    "visclassifier(lambda X:evalforest(trees,X),xTr2d,yTr)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTr2d)) != yTr))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTe2d)) != yTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>The following script evaluates the test and training error of a random forest ensemble as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "for i in range(M):\n",
    "    trees=forest(xTr,yTr,i+1)\n",
    "    trErr = np.mean(np.sign(evalforest(trees,xTr)) != yTr)\n",
    "    teErr = np.mean(np.sign(evalforest(trees,xTe)) != yTe)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "plt.title(\"Random Forest\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Boosting</h2>\n",
    "\n",
    "<p>Another option to improve your decision trees is to build trees of small depth (e.g. only depth=3 or depth=4). These do not have high variance, but instead suffer from <b>high bias</b>. You can reduce the bias of a classifier with boosting. Implement the function <code>boosttree</code>, which applies Adaboost on your <code>cart</code> functions. You should be able to use the function <code>evalforest</code> to evaluate your boosted ensemble (provdided you pass on the weights correctly.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def boosttree(x,y,maxiter=100,maxdepth=2):\n",
    "    \"\"\"Learns a boosted decision tree.\n",
    "    \n",
    "    Input:\n",
    "        x:        n x d matrix of data points\n",
    "        y:        n-dimensional vector of labels\n",
    "        maxiter:  maximum number of trees\n",
    "        maxdepth: maximum depth of a tree\n",
    "        \n",
    "    Output:\n",
    "        forest: list of TreeNode decision trees of length m\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    (note, m is at most maxiter, but may be smaller,\n",
    "    as dictated by the Adaboost algorithm)\n",
    "    \"\"\"\n",
    "    assert np.allclose(np.unique(y), np.array([-1,1])); # the labels must be -1 and 1 \n",
    "    n,d = x.shape\n",
    "    weights = np.ones(n) / n\n",
    "    preds   = None\n",
    "    forest  = []\n",
    "    alphas  = []\n",
    "\n",
    "    \n",
    "    \n",
    "    return forest, alphas\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p>The following script evaluates the test and training error of a boosted forest as we increase the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "for i in range(M):\n",
    "    forest,alphas=boosttree(xTr,yTr,maxdepth=3,maxiter=i+1)\n",
    "    trErr = np.mean(np.sign(evalforest(forest,xTr,alphas)) != yTr)\n",
    "    teErr = np.mean(np.sign(evalforest(forest,xTe,alphas)) != yTe)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "plt.title(\"Adaboost\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
